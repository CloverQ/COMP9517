{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddade850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4911f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels),(test_images, test_labels)= fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809b5ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training images: 60000\n",
      "training labels: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"training images: {len(train_images)}\")\n",
    "print(f\"training labels: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9c44b",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d9299",
   "metadata": {},
   "source": [
    "- training set: 3000\n",
    "- test set: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f21400",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images[: 3000]\n",
    "y_train = train_labels[: 3000]\n",
    "x_test = test_images[: 1000]\n",
    "y_test = test_labels[: 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a808473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape: 3D -> 2D\n",
    "def reshape(dataset): \n",
    "    num, x_value, y_value = dataset.shape\n",
    "    dataset = dataset.reshape((num, x_value * y_value))\n",
    "    return dataset\n",
    "x_train = reshape(x_train)\n",
    "x_test = reshape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1767bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8aed41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predict_y = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "253704c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.806\n",
      "precision score:  0.8174333849677498\n",
      "recall score:  0.8083572787387018\n",
      "f1 score:  0.8102244121587207\n"
     ]
    }
   ],
   "source": [
    "# report\n",
    "accuracy = metrics.accuracy_score(y_test, predict_y)\n",
    "precision = metrics.precision_score(y_test, predict_y, average = \"macro\")\n",
    "recall = metrics.recall_score(y_test, predict_y, average = \"macro\")\n",
    "f1 = metrics.f1_score(y_test, predict_y, average = \"macro\")\n",
    "print(\"accuracy score: \", accuracy)\n",
    "print(\"precision score: \", precision)\n",
    "print(\"recall score: \", recall)\n",
    "print(\"f1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fa93c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.82      0.76       107\n",
      "           1       0.92      0.96      0.94       105\n",
      "           2       0.64      0.68      0.66       111\n",
      "           3       0.86      0.74      0.80        93\n",
      "           4       0.76      0.70      0.73       115\n",
      "           5       0.97      0.77      0.86        87\n",
      "           6       0.59      0.59      0.59        97\n",
      "           7       0.85      0.93      0.89        95\n",
      "           8       0.99      0.93      0.96        95\n",
      "           9       0.88      0.96      0.91        95\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.82      0.81      0.81      1000\n",
      "weighted avg       0.81      0.81      0.81      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(y_test, predict_y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48945c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 88   3   5   2   0   0   9   0   0   0]\n",
      " [  2 101   0   2   0   0   0   0   0   0]\n",
      " [  6   1  76   0  14   0  14   0   0   0]\n",
      " [ 10   4   1  69   4   0   5   0   0   0]\n",
      " [  1   1  17   5  81   0  10   0   0   0]\n",
      " [  0   0   1   0   0  67   0  11   1   7]\n",
      " [ 16   0  14   2   8   0  57   0   0   0]\n",
      " [  0   0   0   0   0   1   0  88   0   6]\n",
      " [  1   0   4   0   0   1   1   0  88   0]\n",
      " [  0   0   0   0   0   0   0   4   0  91]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, predict_y)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24825e7",
   "metadata": {},
   "source": [
    "## KNN Classifier\n",
    "- training set: 6000\n",
    "- test set: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "499cee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images[: 6000]\n",
    "y_train = train_labels[: 6000]\n",
    "x_test = test_images[: 1000]\n",
    "y_test = test_labels[: 1000]\n",
    "x_train = reshape(x_train)\n",
    "x_test = reshape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43f7303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(x_train, y_train)\n",
    "# predict\n",
    "predict_y = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df83a453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.816\n",
      "precision score:  0.8261289198532291\n",
      "recall score:  0.8173032626211448\n",
      "f1 score:  0.8186216959401345\n"
     ]
    }
   ],
   "source": [
    "# report\n",
    "accuracy = metrics.accuracy_score(y_test, predict_y)\n",
    "precision = metrics.precision_score(y_test, predict_y, average = \"macro\")\n",
    "recall = metrics.recall_score(y_test, predict_y, average = \"macro\")\n",
    "f1 = metrics.f1_score(y_test, predict_y, average = \"macro\")\n",
    "print(\"accuracy score: \", accuracy)\n",
    "print(\"precision score: \", precision)\n",
    "print(\"recall score: \", recall)\n",
    "print(\"f1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32cfb0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.83      0.75       107\n",
      "           1       0.95      0.96      0.96       105\n",
      "           2       0.69      0.80      0.74       111\n",
      "           3       0.87      0.77      0.82        93\n",
      "           4       0.80      0.70      0.75       115\n",
      "           5       0.97      0.80      0.88        87\n",
      "           6       0.58      0.52      0.55        97\n",
      "           7       0.88      0.91      0.89        95\n",
      "           8       0.98      0.92      0.95        95\n",
      "           9       0.85      0.96      0.90        95\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.83      0.82      0.82      1000\n",
      "weighted avg       0.82      0.82      0.82      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(y_test, predict_y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7596aef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 89   2   3   2   0   0  10   0   1   0]\n",
      " [  1 101   0   2   1   0   0   0   0   0]\n",
      " [  5   0  89   1   7   0   9   0   0   0]\n",
      " [ 11   2   2  72   2   0   4   0   0   0]\n",
      " [  1   1  17   4  81   0  11   0   0   0]\n",
      " [  0   0   0   0   0  70   0   8   1   8]\n",
      " [ 19   0  16   2  10   0  50   0   0   0]\n",
      " [  0   0   0   0   0   1   0  86   0   8]\n",
      " [  3   0   2   0   0   1   2   0  87   0]\n",
      " [  0   0   0   0   0   0   0   4   0  91]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, predict_y)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a2778",
   "metadata": {},
   "source": [
    "## KNN Classifier\n",
    "- training set: 12000\n",
    "- test set: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd530313",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images[: 12000]\n",
    "y_train = train_labels[: 12000]\n",
    "x_test = test_images[: 1000]\n",
    "y_test = test_labels[: 1000]\n",
    "x_train = reshape(x_train)\n",
    "x_test = reshape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d014a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(x_train, y_train)\n",
    "# predict\n",
    "predict_y = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39d47334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.812\n",
      "precision score:  0.823805187188495\n",
      "recall score:  0.813877009165226\n",
      "f1 score:  0.8155680152573896\n"
     ]
    }
   ],
   "source": [
    "# report\n",
    "accuracy = metrics.accuracy_score(y_test, predict_y)\n",
    "precision = metrics.precision_score(y_test, predict_y, average = \"macro\")\n",
    "recall = metrics.recall_score(y_test, predict_y, average = \"macro\")\n",
    "f1 = metrics.f1_score(y_test, predict_y, average = \"macro\")\n",
    "print(\"accuracy score: \", accuracy)\n",
    "print(\"precision score: \", precision)\n",
    "print(\"recall score: \", recall)\n",
    "print(\"f1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfa13057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.82      0.74       107\n",
      "           1       0.99      0.96      0.98       105\n",
      "           2       0.69      0.80      0.74       111\n",
      "           3       0.84      0.76      0.80        93\n",
      "           4       0.79      0.67      0.72       115\n",
      "           5       0.97      0.80      0.88        87\n",
      "           6       0.59      0.54      0.56        97\n",
      "           7       0.86      0.91      0.88        95\n",
      "           8       0.99      0.92      0.95        95\n",
      "           9       0.86      0.96      0.91        95\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.82      0.81      0.82      1000\n",
      "weighted avg       0.82      0.81      0.81      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(y_test, predict_y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9544af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 88   0   5   3   0   0  10   0   1   0]\n",
      " [  2 101   0   2   0   0   0   0   0   0]\n",
      " [  6   0  89   1   7   0   8   0   0   0]\n",
      " [ 13   1   2  71   4   0   2   0   0   0]\n",
      " [  1   0  18   6  77   0  13   0   0   0]\n",
      " [  0   0   0   0   0  70   0  10   0   7]\n",
      " [ 19   0  14   2  10   0  52   0   0   0]\n",
      " [  0   0   0   0   0   1   0  86   0   8]\n",
      " [  3   0   1   0   0   1   3   0  87   0]\n",
      " [  0   0   0   0   0   0   0   4   0  91]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, predict_y)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc95b8",
   "metadata": {},
   "source": [
    "## DT Classifier\n",
    "- training set: 3000\n",
    "- test set: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c59dbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images[: 3000]\n",
    "y_train = train_labels[: 3000]\n",
    "x_test = test_images[: 1000]\n",
    "y_test = test_labels[: 1000]\n",
    "x_train = reshape(x_train)\n",
    "x_test = reshape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d8dd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(x_train, y_train)\n",
    "# predict\n",
    "predict_y = DT.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6218ce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.703\n",
      "precision score:  0.7063045474456715\n",
      "recall score:  0.7085655807741871\n",
      "f1 score:  0.7071533142230422\n"
     ]
    }
   ],
   "source": [
    "# report\n",
    "accuracy = metrics.accuracy_score(y_test, predict_y)\n",
    "precision = metrics.precision_score(y_test, predict_y, average = \"macro\")\n",
    "recall = metrics.recall_score(y_test, predict_y, average = \"macro\")\n",
    "f1 = metrics.f1_score(y_test, predict_y, average = \"macro\")\n",
    "print(\"accuracy score: \", accuracy)\n",
    "print(\"precision score: \", precision)\n",
    "print(\"recall score: \", recall)\n",
    "print(\"f1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6afc7d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.65      0.67       107\n",
      "           1       0.86      0.91      0.88       105\n",
      "           2       0.59      0.57      0.58       111\n",
      "           3       0.67      0.65      0.66        93\n",
      "           4       0.55      0.54      0.55       115\n",
      "           5       0.83      0.82      0.82        87\n",
      "           6       0.40      0.41      0.41        97\n",
      "           7       0.81      0.85      0.83        95\n",
      "           8       0.81      0.84      0.82        95\n",
      "           9       0.87      0.84      0.86        95\n",
      "\n",
      "    accuracy                           0.70      1000\n",
      "   macro avg       0.71      0.71      0.71      1000\n",
      "weighted avg       0.70      0.70      0.70      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(y_test, predict_y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f3d0589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70  4  4  3  2  1 21  0  2  0]\n",
      " [ 1 96  1  5  1  1  0  0  0  0]\n",
      " [ 6  0 63  4 24  0 11  0  3  0]\n",
      " [ 6  8  1 60  5  1  9  0  3  0]\n",
      " [ 2  2 22  6 62  0 15  0  6  0]\n",
      " [ 0  1  0  1  0 71  0 11  1  2]\n",
      " [17  0 12  7 17  0 40  0  4  0]\n",
      " [ 0  0  0  0  0  5  0 81  0  9]\n",
      " [ 1  1  3  2  1  2  3  1 80  1]\n",
      " [ 0  0  1  2  0  5  0  7  0 80]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, predict_y)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691744a",
   "metadata": {},
   "source": [
    "## DT Classifier\n",
    "- training set: 6000\n",
    "- test set: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21455dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images[: 6000]\n",
    "y_train = train_labels[: 6000]\n",
    "x_test = test_images[: 1000]\n",
    "y_test = test_labels[: 1000]\n",
    "x_train = reshape(x_train)\n",
    "x_test = reshape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58f961ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(x_train, y_train)\n",
    "# predict\n",
    "predict_y = DT.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5a55572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.736\n",
      "precision score:  0.7488753382028177\n",
      "recall score:  0.7420363697250281\n",
      "f1 score:  0.7434982630557865\n"
     ]
    }
   ],
   "source": [
    "# report\n",
    "accuracy = metrics.accuracy_score(y_test, predict_y)\n",
    "precision = metrics.precision_score(y_test, predict_y, average = \"macro\")\n",
    "recall = metrics.recall_score(y_test, predict_y, average = \"macro\")\n",
    "f1 = metrics.f1_score(y_test, predict_y, average = \"macro\")\n",
    "print(\"accuracy score: \", accuracy)\n",
    "print(\"precision score: \", precision)\n",
    "print(\"recall score: \", recall)\n",
    "print(\"f1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea93dee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.64      0.68       107\n",
      "           1       0.88      0.92      0.90       105\n",
      "           2       0.57      0.65      0.61       111\n",
      "           3       0.71      0.72      0.72        93\n",
      "           4       0.68      0.55      0.61       115\n",
      "           5       0.89      0.83      0.86        87\n",
      "           6       0.39      0.46      0.42        97\n",
      "           7       0.86      0.87      0.87        95\n",
      "           8       0.91      0.84      0.87        95\n",
      "           9       0.87      0.94      0.90        95\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.75      0.74      0.74      1000\n",
      "weighted avg       0.74      0.74      0.74      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(y_test, predict_y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f52a34d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[68  2  8  3  1  1 24  0  0  0]\n",
      " [ 1 97  0  5  0  1  1  0  0  0]\n",
      " [ 2  0 72  1 15  0 18  2  1  0]\n",
      " [ 6  6  2 67  2  0  9  0  0  1]\n",
      " [ 1  0 30  7 63  0 13  0  1  0]\n",
      " [ 0  1  1  1  1 72  0  7  1  3]\n",
      " [15  2 13  9  9  0 45  0  4  0]\n",
      " [ 0  0  0  0  0  2  0 83  1  9]\n",
      " [ 1  2  1  1  2  3  4  1 80  0]\n",
      " [ 0  0  0  0  0  2  1  3  0 89]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, predict_y)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1395d0",
   "metadata": {},
   "source": [
    "## DT Classifier\n",
    "- training set: 12000\n",
    "- test set: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80ed3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images[: 12000]\n",
    "y_train = train_labels[: 12000]\n",
    "x_test = test_images[: 1000]\n",
    "y_test = test_labels[: 1000]\n",
    "x_train = reshape(x_train)\n",
    "x_test = reshape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a51c1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(x_train, y_train)\n",
    "# predict\n",
    "predict_y = DT.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b548164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.768\n",
      "precision score:  0.775462783233327\n",
      "recall score:  0.7716259544249555\n",
      "f1 score:  0.7723449614000295\n"
     ]
    }
   ],
   "source": [
    "# report\n",
    "accuracy = metrics.accuracy_score(y_test, predict_y)\n",
    "precision = metrics.precision_score(y_test, predict_y, average = \"macro\")\n",
    "recall = metrics.recall_score(y_test, predict_y, average = \"macro\")\n",
    "f1 = metrics.f1_score(y_test, predict_y, average = \"macro\")\n",
    "print(\"accuracy score: \", accuracy)\n",
    "print(\"precision score: \", precision)\n",
    "print(\"recall score: \", recall)\n",
    "print(\"f1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea5d4bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       107\n",
      "           1       0.89      0.96      0.92       105\n",
      "           2       0.63      0.71      0.67       111\n",
      "           3       0.76      0.67      0.71        93\n",
      "           4       0.68      0.62      0.65       115\n",
      "           5       0.89      0.87      0.88        87\n",
      "           6       0.54      0.59      0.56        97\n",
      "           7       0.85      0.86      0.85        95\n",
      "           8       0.90      0.83      0.86        95\n",
      "           9       0.86      0.87      0.86        95\n",
      "\n",
      "    accuracy                           0.77      1000\n",
      "   macro avg       0.78      0.77      0.77      1000\n",
      "weighted avg       0.77      0.77      0.77      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(y_test, predict_y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a96925e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 78   1   3   6   1   1  15   0   2   0]\n",
      " [  1 101   0   2   1   0   0   0   0   0]\n",
      " [  4   0  79   1  17   0  10   0   0   0]\n",
      " [  7  11   3  62   2   0   7   0   1   0]\n",
      " [  2   1  24   5  71   0  12   0   0   0]\n",
      " [  0   0   0   0   0  76   0   5   2   4]\n",
      " [  9   0  13   3  11   0  57   0   3   1]\n",
      " [  0   0   0   0   0   5   0  82   0   8]\n",
      " [  0   0   4   3   1   0   5   2  79   1]\n",
      " [  0   0   0   0   0   3   0   8   1  83]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, predict_y)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddedb98",
   "metadata": {},
   "source": [
    "## SGD Classifier\n",
    "- training set: 3000\n",
    "- test set: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d82c49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images[: 3000]\n",
    "y_train = train_labels[: 3000]\n",
    "x_test = test_images[: 1000]\n",
    "y_test = test_labels[: 1000]\n",
    "x_train = reshape(x_train)\n",
    "x_test = reshape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45b985f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data\n",
    "SGD = SGDClassifier(max_iter = 250)\n",
    "SGD.fit(x_train, y_train)\n",
    "# predict\n",
    "predict_y = SGD.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0dc4d220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.777\n",
      "precision score:  0.7941135956757345\n",
      "recall score:  0.7810455456193656\n",
      "f1 score:  0.7820609433534768\n"
     ]
    }
   ],
   "source": [
    "# report\n",
    "accuracy = metrics.accuracy_score(y_test, predict_y)\n",
    "precision = metrics.precision_score(y_test, predict_y, average = \"macro\")\n",
    "recall = metrics.recall_score(y_test, predict_y, average = \"macro\")\n",
    "f1 = metrics.f1_score(y_test, predict_y, average = \"macro\")\n",
    "print(\"accuracy score: \", accuracy)\n",
    "print(\"precision score: \", precision)\n",
    "print(\"recall score: \", recall)\n",
    "print(\"f1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11d3f687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.61      0.73       107\n",
      "           1       0.89      0.95      0.92       105\n",
      "           2       0.58      0.80      0.67       111\n",
      "           3       0.77      0.76      0.77        93\n",
      "           4       0.69      0.62      0.65       115\n",
      "           5       0.89      0.82      0.85        87\n",
      "           6       0.53      0.54      0.53        97\n",
      "           7       0.91      0.88      0.90        95\n",
      "           8       0.86      0.93      0.89        95\n",
      "           9       0.91      0.91      0.91        95\n",
      "\n",
      "    accuracy                           0.78      1000\n",
      "   macro avg       0.79      0.78      0.78      1000\n",
      "weighted avg       0.79      0.78      0.78      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(y_test, predict_y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51cff537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 65   4  10   9   2   0  15   0   2   0]\n",
      " [  0 100   1   4   0   0   0   0   0   0]\n",
      " [  1   1  89   2   7   0  11   0   0   0]\n",
      " [  1   5   5  71   6   0   5   0   0   0]\n",
      " [  0   1  28   3  71   0  11   0   1   0]\n",
      " [  0   1   1   0   0  71   0   4   5   5]\n",
      " [  4   0  20   2  16   0  52   0   3   0]\n",
      " [  0   0   0   0   0   6   0  84   1   4]\n",
      " [  0   0   0   1   1   0   5   0  88   0]\n",
      " [  0   0   0   0   0   3   0   4   2  86]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, predict_y)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46a8f1",
   "metadata": {},
   "source": [
    "## SGD Classifier\n",
    "- training set: 6000\n",
    "- test set: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79a729ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images[: 6000]\n",
    "y_train = train_labels[: 6000]\n",
    "x_test = test_images[: 1000]\n",
    "y_test = test_labels[: 1000]\n",
    "x_train = reshape(x_train)\n",
    "x_test = reshape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bfdf9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data\n",
    "SGD = SGDClassifier(max_iter = 250)\n",
    "SGD.fit(x_train, y_train)\n",
    "# predict\n",
    "predict_y = SGD.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c98f6dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.759\n",
      "precision score:  0.8029436110222976\n",
      "recall score:  0.7624172595010829\n",
      "f1 score:  0.7606477463461297\n"
     ]
    }
   ],
   "source": [
    "# report\n",
    "accuracy = metrics.accuracy_score(y_test, predict_y)\n",
    "precision = metrics.precision_score(y_test, predict_y, average = \"macro\")\n",
    "recall = metrics.recall_score(y_test, predict_y, average = \"macro\")\n",
    "f1 = metrics.f1_score(y_test, predict_y, average = \"macro\")\n",
    "print(\"accuracy score: \", accuracy)\n",
    "print(\"precision score: \", precision)\n",
    "print(\"recall score: \", recall)\n",
    "print(\"f1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ed56f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.87      0.73       107\n",
      "           1       0.95      0.95      0.95       105\n",
      "           2       0.49      0.88      0.63       111\n",
      "           3       0.92      0.48      0.63        93\n",
      "           4       0.80      0.38      0.52       115\n",
      "           5       0.92      0.87      0.89        87\n",
      "           6       0.54      0.46      0.50        97\n",
      "           7       0.93      0.87      0.90        95\n",
      "           8       0.93      0.89      0.91        95\n",
      "           9       0.91      0.95      0.93        95\n",
      "\n",
      "    accuracy                           0.76      1000\n",
      "   macro avg       0.80      0.76      0.76      1000\n",
      "weighted avg       0.80      0.76      0.75      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(y_test, predict_y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3672bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 93   1   8   0   0   0   4   0   1   0]\n",
      " [  2 100   0   3   0   0   0   0   0   0]\n",
      " [  2   1  98   1   1   0   8   0   0   0]\n",
      " [ 24   3   7  45   4   0   9   0   1   0]\n",
      " [  5   0  54   0  44   0  12   0   0   0]\n",
      " [  0   0   3   0   0  76   1   3   1   3]\n",
      " [ 17   0  28   0   5   0  45   0   2   0]\n",
      " [  0   0   0   0   0   5   0  83   1   6]\n",
      " [  4   0   1   0   1   0   4   0  85   0]\n",
      " [  0   0   0   0   0   2   0   3   0  90]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, predict_y)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10e93a",
   "metadata": {},
   "source": [
    "## SGD Classifier\n",
    "- training set: 12000\n",
    "- test set: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1abbc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images[: 12000]\n",
    "y_train = train_labels[: 12000]\n",
    "x_test = test_images[: 1000]\n",
    "y_test = test_labels[: 1000]\n",
    "x_train = reshape(x_train)\n",
    "x_test = reshape(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b379ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data\n",
    "SGD = SGDClassifier(max_iter = 250)\n",
    "SGD.fit(x_train, y_train)\n",
    "# predict\n",
    "predict_y = SGD.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0cca97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.8\n",
      "precision score:  0.8220317828922044\n",
      "recall score:  0.8038901997993768\n",
      "f1 score:  0.8090833470077957\n"
     ]
    }
   ],
   "source": [
    "# report\n",
    "accuracy = metrics.accuracy_score(y_test, predict_y)\n",
    "precision = metrics.precision_score(y_test, predict_y, average = \"macro\")\n",
    "recall = metrics.recall_score(y_test, predict_y, average = \"macro\")\n",
    "f1 = metrics.f1_score(y_test, predict_y, average = \"macro\")\n",
    "print(\"accuracy score: \", accuracy)\n",
    "print(\"precision score: \", precision)\n",
    "print(\"recall score: \", recall)\n",
    "print(\"f1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "618346bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.76      0.80       107\n",
      "           1       0.94      0.96      0.95       105\n",
      "           2       0.69      0.70      0.70       111\n",
      "           3       0.76      0.77      0.77        93\n",
      "           4       0.78      0.67      0.72       115\n",
      "           5       0.93      0.85      0.89        87\n",
      "           6       0.45      0.66      0.54        97\n",
      "           7       0.92      0.93      0.92        95\n",
      "           8       0.97      0.78      0.87        95\n",
      "           9       0.93      0.96      0.94        95\n",
      "\n",
      "    accuracy                           0.80      1000\n",
      "   macro avg       0.82      0.80      0.81      1000\n",
      "weighted avg       0.82      0.80      0.81      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(y_test, predict_y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb260cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 81   2   1   4   0   0  18   0   1   0]\n",
      " [  0 101   0   4   0   0   0   0   0   0]\n",
      " [  3   0  78   1   9   0  20   0   0   0]\n",
      " [  3   4   3  72   4   0   7   0   0   0]\n",
      " [  0   0  17   6  77   0  15   0   0   0]\n",
      " [  0   0   0   2   0  74   3   4   0   4]\n",
      " [  7   0  13   4   8   0  64   0   1   0]\n",
      " [  0   0   0   0   0   4   0  88   0   3]\n",
      " [  1   0   1   2   1   2  14   0  74   0]\n",
      " [  0   0   0   0   0   0   0   4   0  91]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, predict_y)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891244c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- By comparing the experimental results in the paper, we can see that our accuracy is slightly lower. This is because in the SGD classifier, the max_iter we choose is 250, and the max_iter in the paper is 1000, so there is underfitting in our model.\n",
    "- By comparing the three classifiers, KNN has the best classification effect, followed by SGD and DT.\n",
    "- The increase of training set will increase the accuracy of the model slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ab0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
